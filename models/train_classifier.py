# Import Libraries
import sys
import nltk
nltk.download(['punkt', 'wordnet'])

import re
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
import joblib

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


def load_data(database_filepath):
    '''
    This function reads in the sql table from the given database file,
    splits the dataframe into a dataframe of features (X) containing messages
    and a dataframe of 36 categories (y).
    It also extracts and returns the 36 category names for printing the results
    
    Parameters
    ----------
    database_filpath : Data Frame

    Returns
    -------
    X : Dataframe of messages
    y : Dataframe of categories
    target_names: Category names

    '''
    engine = create_engine('sqlite:///' + database_filepath)
    df = pd.read_sql_table('DRResponse_Ajay', engine)
    X = df['message'].values
    y = df.drop(['id', 'message', 'original', 'genre'], axis = 1).values
    target_names = list(df.drop(['id', 'message', 'original', 'genre'], axis = 1).columns)
    return X, y, target_names


def tokenize(text):
    '''
    This function tokenizes given text including splitting, lemmatizing and
    removing stop words
    
    Parameters
    ----------
    text : Message passed as string

    Returns
    -------
    clean_tokens : Tokenized message

    '''
    tokens = word_tokenize(text)
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    
    clean_tokens = []
    for tok in tokens:
        clean_tok = lemmatizer.lemmatize(tok).lower().strip()
        clean_tok = [w for w in clean_tok if w not in stopwords.words("english")]
        clean_tokens.append(clean_tok)
    
    return clean_tokens


def build_model():
    '''
    This function returns a model pipeline.
    The model pipeline is of two layers. 
    The first layer is a transformer that tokenizes the message, while the 
    second layer is a multi output classifier.
    
    The reason for setting it up as two layer pipeline is to enable saving
    the pipeline as a pickel file.
    
    The best parameters is selected using sklearn grid search.
    
    Parameters
    ----------
    None

    Returns
    -------
    cv : Pipeline with grid search

    '''
    model_pipeline = Pipeline([
        ('nlp_pipeline', Pipeline([
                ('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
            ])),
        ('clf', MultiOutputClassifier(KNeighborsClassifier())),
    ])  

    # define parameters for GridSearchCV
    parameters = {
        'clf__estimator__leaf_size': [10, 30],
        'clf__estimator__n_neighbors': [5, 10],
        #'clf__estimator__p': [2, 3],
    }

    # create grid search object
    cv = GridSearchCV(model_pipeline, param_grid = parameters) 

    return cv

def display_results(y_test, y_pred, target_names):
    '''
    This function reads in the actual categories from the test data, and the
    predicted categories generated by the model on the test data.
    It also reads in the category name list to print out F1 score, precision
    and recall using classification_report from sklearn
    
    Parameters
    ----------
    y_test : actual categories in the test data
    y_pred : predicted categories from the model fit on the test data
    target_names : category names

    Returns
    -------
    None

    '''
    i = 0
    for l in target_names:
        print('Score for {}:\n'.format(l))
        print(classification_report(y_pred[i], y_test[i]))
        i += 1
        
        
def evaluate_model(model, X_test, Y_test, category_names):
    '''
    This function predicts the categories on the test data, calls the
    'display_results' to print the F1 score, precision and recall per category,
    and the best parameters determined by the Grid Search.
    
    Parameters
    ----------
    model : The model fit on training data
    X_test : The messages in the test data that will be used to predict categories
    Y_test : The actual categories associated with the test data
    category_names : The names of 36 categories
    

    Returns
    -------
    None

    '''
    y_pred = model.predict(X_test)
    display_results(Y_test, y_pred, category_names)
    print("\nBest Parameters:", model.best_params_)


def save_model(model, model_filepath):
    '''
    This function saves the model as pickel file
    
    Parameters
    ----------
    model : The final model to be saved
    model_filepath : The path where pickel file will be saved


    Returns
    -------
    None

    '''
    joblib.dump(model, model_filepath)


def main():
    if len(sys.argv) == 3:
        database_filepath, model_filepath = sys.argv[1:]
        print('Loading data...\n    DATABASE: {}'.format(database_filepath))
        X, Y, category_names = load_data(database_filepath)
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
        
        print('Building model...')
        model = build_model()
        
        print('Training model...')
        model.fit(X_train, Y_train)
        
        print('Evaluating model...')
        evaluate_model(model, X_test, Y_test, category_names)

        print('Saving model...\n    MODEL: {}'.format(model_filepath))
        save_model(model, model_filepath)

        print('Trained model saved!')

    else:
        print('Please provide the filepath of the disaster messages database '\
              'as the first argument and the filepath of the pickle file to '\
              'save the model to as the second argument. \n\nExample: python '\
              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


if __name__ == '__main__':
    main()